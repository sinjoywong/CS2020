 本文从“如何设计一个具有重删功能的全闪存储系统”为引子， 不按知识点来简单罗列，而是以需要什么讲什么的方式来深入展开。 概念性的东西比较多，计划先作为引文，然后里面的有必要深究的知识点再单独整理文章发出。

# 系统整体架构

多节点组成一个集群，节点间为主从架构。每个I/O栈都有相应的逻辑卷，每个逻辑卷都有一个主节点。在节点层面来说，主节点负责读写操作相关流程，从节点负责镜像以实现高可用。

镜像的对象根据需求，可能是直接镜像数据本身（如缓存层），也可能是镜像操作（如元数据、日志卷层）。

实现过程中整体分为配置和业务两大类。配置模块主要是集群来负责，具体使用paxos协议来保证分布式系统中的一致性的达成。每个节点都具有独立的配置，但是通过集群的同步来实现全局配置信息的同步。

业务模块则根据整体架构的设计不同而不同。一般来讲，每个节点都相对独立地进行I/O处理。有状态变更的时候就需要先静默I/O，然后安全地修改配置信息，并将其在集群中的每个节点间进行同步。

首先从整体上说明配置，再说明具体业务，对于理解整体的框架是有益的，因此先将简要地讲述一下集群管理。

# 集群

## 为什么需要集群，而不能单机？

可以从性能和可靠性两个角度考虑。
从性能角度来讲，每个节点作为一个单机，无论配置有多高（更快的CPU、更大的内存、更快的硬盘等等），还是软件优化有多好（更好的多线程调度，等等），其硬件性能始终是有限的。
从可靠性角度来讲则更显然。对于一个单机设备，若发生故障，则会中断业务。集群则可以采取一些高可用的设计使得剩余节点或备份节点来接管故障节点，从而保证业务的连续性和数据的可靠性。

## 节点与集群的关系是什么？

基本思想是配置与业务分离，其中配置信息由所有节点共享，业务为每个节点独立。这样既可以实现集群中每个节点的差异化工作，还可以保证集群中每个节点的配置的统一性（无论是在正常业务还是在单点故障场景下）。

每个节点都有各自的独立的数据进行业务管理。对于整个集群需要维护一份公共的数据作为配置管理，该份公共数据需要实现每个节点的一致性。
显然每个节点是独立的硬件设备，具有独立的CPU、内存。因此需要一套机制来维护多个节点之间公共数据的同步。此处便涉及到了集群间节点的通信方法。

## 【待补充】集群中的节点如何保证数据的一致性

此处已经有很多成熟的集群一致性协议，如PAXOS，基于PAXOS发展而来的Raft等协议。

【待补充】

## 集群的状态机

管理配置信息的变更，目前最好的方式是采用订阅-通知设计模式的状态机转换系统。

状态机指的是，使用一组状态来维护某个对象的一些属性，不同的状态可以有订阅和通知两种触发方式，当其中的某个属性发生变更后，被被通知的函数就会被触发，该函数内可以实现一些逻辑，可以将这些参数作为输入或输出，在逻辑计算完成之后，可以得到新的状态值。

主要操作有以下几种：

1）一个状态机，有一个或多个输入参数，也可以有一个或多个输出参数。输入参数的变更触发状态机。

输入参数可以是订阅型的，即：订阅了参数a。当a发生更改后，将触发状态机逻辑，进入状态机处理函数。也可以是非订阅型的，即：该参数只意味着在该状态机中可以进行读取/写入操作，但它本身的变更不会进入状态机逻辑。

2）一个状态机，有一个或多个输入参数，也可以有一个或多个输出参数。输出参数的变更触发状态机。

同上，只是输入仅作为参数读入，而不会主动触发，只有输出发生更改才会触发状态机逻辑。状态机2比状态机1的特点是，推迟了状态机的触发时间，

## 集群和节点数据的交互

作为一个框架，我们需要设计一种通用的消息收发功能用于日后的开发。
需要有以下三种功能：

### 1）从集群向某个节点发消息

此处可以针对需求进行二次封装，即，基本操作为指定一个节点的唯一编号来进行数据发送。当然想要广播到所有节点的话就可以指定所有节点来实现。

### 2）从某个节点向集群发消息

此处可以针对需求进行二次封装，即，可以是通过单个节点的一个参数的变化直接通知给集群，也可以是使用一个位图的方式，每个节点用其中的一个位来描述，状态发生变更后，同样的也是各自直接发送到集群，然后由集群的逻辑来判断是否状态已变更的节点的数目与当前在线节点数目相同，从而来实现“所有节点都设置了某个位才触发一些逻辑”的功能。

### 【？实现上有什么不同？】3）节点与节点之间直接发消息

节点与节点间的通信常用于业务本身，可以分为发送短消息message和长消息两种。

集群中是如何通信的

那么如何进行节点间通信呢？
我们知道进程间通信主要有管道、消息队列、共享内存、socket通信（还可用于主机-主机）等方式。

基于以上三个基本功能，就可以按照具体业务设计一套通用的框架。

## 主从型架构：归属节点的设计

对于集群来讲，这个集群对外提供一个逻辑对象，但是集群内部还是每个节点各自在工作。在各层软件的设计中，我们需要指明对于该层来讲，其请求是使用哪（几）个节点来处理的。
AB型架构中，假设两个节点组成一个集群，对于一个上层请求来讲，都需要其中的一个节点来处理，另一个节点可以做点别的（例如镜像本节点，或什么都不做只做热备）。这样的话对于每个卷对象来说就需要一个归属节点来表示该请求要使用哪个节点来处理。
在代码实现过程中，我们可以想象出每个节点上都运行着相同的代码，那么如何来实现每个节点进行不同的工作呢？
可以这么实现：
每个节点都由一个全局唯一的编号来区分，在集群管理层中使用一个变量来保存每个节点的存在情况。这个变量可以是一个bitmap（因为一个系统设计时就已经固定了集群中最大节点的数目）。
以四节点集群系统为例，我们可以使用一个包含4个Bit的变量nodes来表示，

节点编号	3	2	1	0
存在标记	0/1	0/1	0/1	0/1
举例说明：若集群中无节点，则该值中每个编号所对应的bit位标记都为0，nodes=0；
若集群中编号为0的节点存在，则该值中编号为0所对应的bit位标记为1，其他仍旧为0，nodes=1;
若集群中编号为0和1的节点都存在，则该值中编号为0和1所对应的bit位标记为1，其他仍旧为0，即nodes=3；
这样每个节点的存在与否都可以通过一个值来说明。以此为基础，我们可以衍生出来许多方法，例如维护一个当前已经稳定运行的在线节点的set，再通过硬件检测来维护一个集群中动态变化的节点的set，对两者按位与则可以知道是否有节点想要加入或退出，此时便可以通过一些自定义事件的回调机制处理一些逻辑，例如暂停当前存活节点各模块任务，准备接管等操作。对这些节点事件的处理便抽象出了节点的生命状态，而后才能考虑状态机的实现。这点将在下个小节说明。
将该配置信息在所有节点中同步，则每个节点都可以知道整个集群中的节点的存在状况，这样也就能知道哪个节点不在集群中了。

int node_set;

int add_node_to_node_set(node_set,node_id){
	return	node_set | (1 << node_id);
}
int remove_node_from_node_set(node_set,node_id){
    return node_set & ~(1<<node+id);
}
bool is_node_in_node_set(node_set,node_id){
	return ((node_set &(1<<node_id))!=0);
}
集群管理层	集群管理层
节点A	节点B
节点的生命状态与状态机

集群中多个节点的管理是个复杂的工作，因为要考虑许多东西：集群的建立，节点的加入、离开，其中节点的加入和离开又需考虑正常和异常的情况，这就又涉及到一些组合事件序列的处理。因此首先需要建立节点生命状态，然后才能基于其状态的转换建立其状态机的管理。
当然判断节点在集群中的可用性是个十分复杂的工作。由于分布式系统中的网络分区性，如果只是用心跳线检测来决定是否某个节点在不在线是不准确的，因为可能只是网络阻塞无法即时通信，这时候就需要一些其他的手段。
例如Redis中的Sentinel分为了检测主观下线状态和检测客观下线两个角度(【2】p234)来考虑。
1.主观下线：Sentinel还是以每秒一次的频率向所有与它创建了命令连接的实例（包括主服务器、从服务器、其他Sentinel在内）发送PING命令，并通过实例返回的回复来判断实例是否在线。若一个实例在指定的时间内一直向Sentinel返回无效回复，则Sentinel会判断该实例为主观下线状态，修改该实例对应的属性标记为主观下线。
2.客观下线：由于主观下线的判断是“独裁的”，可能是不准确的，因此最好的办法还是听听其他Sentinel的意见。因此又引入一个客观下线的判断。其实现是基于一个quorum仲裁的方法，简单来讲，当认为某个主服务器已经进入下线状态的Sentinel个数超过了配置中的一个quorum值，那么就认为这个主服务器已经进入客观下线状态。
当然之后的故障接管就又涉及到了一个选举的过程，一般采用的是PAXOS算法，此处先不展开。

# I/O栈配置管理

## 多层管理：

转发层、缓存层、精简卷层、日志卷层、存储池层、RAID层、驱动层、物理磁盘层

## I/O路径的生产者-消费者模型

## 线程模型

基本原则： 每个卷绑定到一个线程中，避免处理多个卷的IO请求时，需要频繁加锁去锁的耗费资源的操作。

## 静默管理：每层的水龙头

为什么需要静默管理呢？ 试想一个主机一直在接收请求，在整个I/O栈的每一层都可能有正在处理的请求，它们各自都依赖于一些配置信息有条不紊地进行处理。
突然发生了一个事件，这个事件需要更改各层的一些配置信息。毫无疑问，直接更改时不行的，只有当各层“暂停”处理请求后，没有正在处理的请求，才能安全地更改。
那么就需要一套机制，来实现以下几项功能：
1。监测事件的发生。这是典型的事件发布-订阅模式。事件发生时要能够通知各层，各层才能进行处理。事件相关的整理见博客事件驱动设计模式。
2。各层在接收到事件后，尽快将本模块的流程中的请求清空，从而达到静默状态，从而允许整个系统尽快地进入静默状态。
为什么要尽快呢？因为在理想中我们希望这些事件不影响主机的I/O，即主机在这个时候还可以正常下发I/O。当然此时实际上各层并不具备处理I/O请求的能力。处理的方式就是使用一个队列来将请求暂存，对于上层来说请求可以下发，只不过没有回应。这个队列会在静默完成之后按照正常I/O请求的方式重新下发，下层再正常进行处理。
“尽快”如何实现？这就看各层的业务逻辑了。原则上就是将正在申请内存的请求释放掉，正在进行转发的请求释放掉，耗时的刷盘暂停掉。这些未完成的请求由事务模块刚开始便加入一个链表，保证在完成前不释放，所以此处的释放请求不会对数据的完整性造成影响。
3。各层静默处理结束后，此时请求追踪器计数应该是0，意味着本层的处理都已经结束，都给上层返回了一个结果，无论是成功还是失败。这时就可以切换配置信息。切换完成之后通知上层静默结束，然后会重新下发加入队列的请求以及主机的请求。
那么如何来追踪这些请求的完成情况呢？请看下文：

### 多层模型中请求完成情况的追踪

按照业务进行划分，每层都需要一些处理，处理完成后将请求发给下层，下层完成后将请求结果返回给上层。这是一种典型的分层设计思路。
显然我们需要一种机制来描述这些请求的返回情况。这是很有用的，我们可以通过这个设计来知道某层下发的请求是否已被完全返回。

### 多层静默设计

考虑多层间静默的实现，就要首先理清楚I/O路径。从主机端下发的请求，有回写和透写两种方式。回写是指该请求直接存储在缓存中，然后缓存层直接向上层返回结果。而是否落盘持久化，则取决于缓存的策略。达到缓存下刷的条件后，经过每层的处理后，再将处理后的请求发给下层，依次向下层传递，一直到最底层（磁盘驱动）。得到磁盘的返回结果，再依次向上返回，直到主机。这是一个完整的I/O路径。

考虑静默的话，就需要维护多层间的请求处理链表。可以根据一些错误码来描述一些场景。例如，当请求到达某层时，可能是刚到达该层的入口，也可能是该层的中间某个master端的处理流程，也可能是盖层中的在slave端的处理流程，也可能是已经返回。这就需要逐一考虑：

1）到达该层的入口时，可以判断当前卷是否正在请求静默。若是，则直接加入一个静默队列，然后向上返回WAIT，表示该请求需要等待下层处理。此时，上层拿到该请求的WAIT标记，会一直向上层传递，主机端什么也不会做。

2）到达master端的中间处理流程时，此时可能有以下操作：事务模块正在更新本端的写缓存，事务模块正在镜像写缓存（会将其加入到一个sync队列中，然后向slave端同步请求，等slave端处理完成后会有应答，master收到应答后再将该请求从sync队列中摘除），写缓存模块正在下刷操作。这时候应该的操作是将停止向对端转发操作，停止等待对端的响应，停止正在下刷的操作。这些请求将会在本端保留，将在合适的时机重新处理。

# 重删的设计

## 离线重删

这种重删方式是在后台定时将数据读取并计算，由于这样的磁盘访问会造成SSD的性能问题，因此一般情况下不会在全闪存储系统中使用。

## 在线重删

1. 数据下发到SE的某个L0时，根据数据计算hash值H0，并查询对应HP树元数据。

> 数据块的粒度是8KB，不采用4KB也是一方面考虑数据拆分时的计算损耗，一方面也是考虑元数据量的大小。显然8KB就比4KB减少了一半的元数据量，这样在元数据的缓存量和下刷量也会对性能有所优化。
>
>  此处使用弱哈希算法，若哈希值相同，则做逐字节匹配确认。使用弱哈希算法也是基于性能的考虑。

2. 查询的过程中，将会先层元数据的写缓存模块查询，因为此处的数据是未下刷的最新的元数据；若查不到，会到读缓存中查询；若还查不到，会进行读盘查询，并将刚读上来的B+树节点保存到读缓存中。

> 写缓存为双控镜像模式，这意味着每次事务的操作需要双控镜像完成之后才算结束，这样可以保证系统的高可用：在发生单节点故障的情况下，写缓存的数据也不会丢失。
>
> 当然，在双节点同时发生故障的情况就需要一些额外的软硬件来保证，例如BBU和守护进程来监控，若主进程挂掉，守护进程便开始将被标记为保护的内存页下刷到内部磁盘中。
>
> 此处固定为内部磁盘的原因是，主进程故障还可能导致外接机柜相关协议栈发生故障，因此尽量不使用高级特性，而使用基础特性更安全一点。
>
> 而读缓存则无需镜像，因为这只是一个加速模块，即使故障丢失也不影响系统功能（会从盘中读取元数据）。反言之，让两个控制器根据owner划分，分别保存不同的元数据，能够在有限的内存限制下保存更多的元数据，起到更好的加速效果。

3. 根据查询结果：

   3.1 若该H对应的P不存在，则说明没有重复数据，此时按照新的数据处理。将调用日志卷管理系统，下刷数据，从而获得一个新的PBA（此处假设为P0），返回给元数据层，在该LBA（此处为L0）对应的B+树中通过事务模块插入L0P0，同时增加一个P0L0（用于垃圾回收），事务模块将会将该修改提交给写缓存模块进行缓存，写缓存模块将会自行决定是否下刷内存中的元数据。

   3.2 若该H对应的P存在（假设为如1中所说P0），则说明已经有重复数据，该重复数据之前被保存在数据卷的P0位置，此时无需再次将该数据保存，而是只需要保存一个映射关系即可，即在该逻辑地址所属的LP树中，增加一组L1P0，然后在该池的PL树中，增加一个P0L1。此时，PL树中就有了P0L0和P0L1两组映射关系。这也是重删功能对元数据模块带来的最明显的现象：一个P对应多个L。

> 对于P多L的现象，还涉及到了一种特殊情况的处理：对于全零数据（格式化卷时下发），在开启重删的情况下会造成全部L对应一个P的情况--全零计算出来的哈希值都相同，这样会造成异常。因此在I/O流程中，对全零数据需要特殊处理，即不去走重删的流程。

## 数据存取位置的管理：元数据

### 读流程

1. 主机把读命令通过FC或iscsi接口发送给精简卷层，请求包含数据块逻辑地址和长度。
2. 精简卷层processTracker++。
3. 精简卷层查询元数据LP。先查询写缓存，再查询读缓存，再读meta卷查询（之后将结果保存到读缓存中）。
4. 精简卷层获得PBA，调用日志卷，传入PBA，在data卷中读取数据，返回给精简卷层。
5. 精简卷层processTracker--。

### 写流程

1. 精简卷层收到clb，申请对应结构体资源，进行数据拆分，将其拆分成8KB大小的数据块。
2. 全零数据的处理。避免B+树的异常。
3. 判断是否长度与grainsize不同，可能需要预读或写零对齐。
4. 计算每个数据块的Hash值，并查询HP元数据。若能查到相同的H，则意味着数据是重复的，此时将无需再次写入重复数据，而是将这个新的LBA与之前已有的HP中的PBA建立映射关系。同时需要修改PL对应的值。
5. 3中的修改使用事务来保证原子性。事务的更改将使用写缓存来保护，写缓存将负责事务修改的多节点间镜像与下刷。写缓存返回的部分失败将由事务模块来保证该多个操作的回滚。
6. 

## 垃圾回收是如何进行回收的？

### 垃圾回收的时机：

垃圾回收模块会起后台线程，检查日志卷中的block利用率。若block利用率大于一个阈值的时候，则会触发垃圾回收。此时将该block内的无效数据进行标记，后面会由RAID层进行trim。而对于有效数据则进行迁移，将其搬迁到新的PBA中，并更新元数据LP、PL。

> block利用率的定义：元数据模块更新时会检查Pba的有效性，若无效则会通知给GC，由GC记录。
>
> **PBA有效性的定义** ：在PL树中，若一个P对应的L都不存在，则认为该P是无效的，即该P处对应的数据也是无效的，可以被trim掉。

### 与垃圾回收的互斥

在精简卷层处理一个数据的时候，即针对一个L，对应的P处的数据也可能被GC正在迁移或回收。因此精简卷和GC在处理某个LBA之前，需要先加锁互斥，此处的锁被称为L锁。

**L锁的实现：**具体的实现是一个粒度为池的哈希表，每个请求使用一个node来管理。在一个请求开始处理前，先计算哈希值，然后在该哈希表中查询，若查询不到（为NULL），则意味着没有用户使用，可以直接开始处理；若能查询到，则意味着已经有用户使用，此处需要加到一个链表中，等待该锁释放后再处理。

## 精简卷

线程分配：一个lun使用一个线程来管理，这样便可以避免一个lun由多个线程管理的互斥问题，以提高效率。

对于元数据的卷对象，也使用了同一个卷一个线程。

## 日志卷

### 目的

SSD的随机性能差，增加一层来进行管理待分配的PBA，将精简卷层的随机LBA尽可能地分配顺序大块的PBA，然后提交给下层，直到SSD硬件层，使得SSD层得到的是尽可能顺序的效果。

### 实现

需要考虑可用block的搜索、可用block的申请以及单节点故障时可用block链表的管理。

#### 数据组织方式

使用一个位图，用于记录可用block、已分配block。

分层实现，最上层为block，为128MB，一个block被划分为若干个下刷单元，每个下刷单元被划分为若干个大小不等的span，span由grain组成，grain为日志卷管理的最小单位，数据卷中一个grain为8KB，元数据卷中一个grain为512B。

block为垃圾回收的基本单位。当垃圾回收开始工作时，将遍历扫描所有的block，然后针对每个block，根据P查询元数据是否有对应的有效L，若没有，则进行标记这些数据trim；若有有效数据，则进行迁移。

## 元数据

### 元数据管理

### 常见的元数据的组织形式

元数据的本质实际上就是建立一种k-v的映射关系，支持增删改查的操作。采用B+树来组织，

### 查询

### 插入

### 事务的引入

### 事务的设计：镜像/日志、线程调度

### 事务，单机系统的一致性与分布式系统的一致性

### 多线程调度的实现

### 删除

### 持久化（下刷）

## 垃圾回收

# 高可用

## 高可用的本质是什么？

数据的可靠性是基于以下几点来考虑的：
一：内存是易失性的，这意味着如果我们不做任何保护，当系统发生掉电故障后，内存中的数据就会丢失。
二：整个系统是多层结构，任何时候各层中都有可能有正在处理的请求，那么当系统发生任何软件、硬件故障的时候，如果我们不做处理，就会导致数据不完整或者丢失。

基于以上两点，实际上我们就可以想到高可用的本质就是能够时刻保证系统中内存中正在处理的数据的完整性。

## 高可用的实现

首先应该完成的是，当系统发生故障（无论是软件故障还是硬件故障，包括掉电故障），我们如何来保护当前内存数据不会丢失？

一方面我们可以想到使用新的NVME来通过硬件角度来实现，但是毕竟昂贵，我们还是需要考虑向下兼容的问题：如何来通过软件的方法来实现内存数据的保存？
我们知道，当这个软件发生软件故障的时候，整个进程就会崩溃，那么就无法依赖这个进程来完成。这样，我们就可以合理地想到使用另一个进程来作为这个进程的守护进程，专门来负责做一些主进程无法处理的事情。
此处便是将内存中的数据持久化到磁盘中。
那么此处就涉及到了内存管理：整个系统如何进行内存管理？内存如何申请、分配、释放？如何来表示我们需要将某些内存页刷到磁盘中？又是如何刷到磁盘中的？实现这个功能需要什么样的管理结构（例如刷写到磁盘中就需要考虑一致性校验，之后恢复到内存中是如何完成的？），另外可能还有一些优化，例如这个内存文件的压缩等。

## 内存管理

我们知道申请内存使用malloc，释放内存使用free。这种方式申请内存时是如何处理，有什么坏处以至于很多数据库组件如Redis,memCached,MariaDb都自己封装一层内存管理呢？他们又解决了什么问题？通过什么来解决的？
要明白以上几点就需要先从Linux内存管理说起。

## Linux是如何进行内存管理的？

主要涉及到段页式内存管理方法。考虑以下问题：
为什么使用段页式来寻址？虚拟内存和物理内存如何建立映射关系？寻址范围如何确定？如何根据内存硬件容量的提升来增加寻址范围（多级页表的发展），如何来进行寻址（地址转换）？缺页中断怎么实现？什么是伙伴系统？为什么要发明伙伴系统？内存碎片是怎样产生的（空闲页的查找算法）？如何避免内存碎片过多？如何进行垃圾回收？malloc/free的时候发生了什么？
参考另一篇文章：Linux内存管理

## 为什么需要内存池的形式来自己管理？

如何实现一个内存池？Redis,memCached,MariaDb是怎么做的，相互之间有什么区别？改进了什么？

# 其他特性：快照

对于一个存储系统，我们需要一些其他的高级特性，例如快照功能。
快照是什么呢？实际上就是一个卷在某个时间的备份。要实现快照，就需要先理解数据的存储方式和元数据的管理方式。
要实现某时的备份，最简单的办法就是直接将整个卷中的所有数据进行复制。然而这样显然很不合理：每次进行快照的时候都将整个卷进行复制，严重影响性能，也严重浪费空间。
因此有了一些降低复制数据量的设计，主要的实现一般有两种：COW和ROW。

COW即Copy On Write，写时复制技术。这种方法

参考文献：
【1】 《MariaDB原理与实现》 张金鹏等
【2】 《Redis设计与实现》 黄建宏
【3】 《Unix核心编程》
【4】 《深入理解计算机系统》
————————————————
版权声明：本文为CSDN博主「吾皇斯巴达」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/SinjoyWong/article/details/103505786



# 开发迭代记录

1. 创建流程，通过日志确认。基本I/O流程。
2. 优化定位手段，添加事件流trace。
3. WT/WB的切换暂不支持，使用环境变量的方式来确定。支持卷删除。--> 在早期开发过程中，需要先处理基本的配置管理、I/O流程，涉及到HA的处理暂不投入，因此先使用环境变量的方式固定，而不考虑其切换。
4. 支持Failover功能，包括静默I/O控制、事件流控制（状态机与utmd)、元数据对象恢复、writeMode切换、owner切换、写缓存下刷、事务重做等。
   静默I/O控制是整个过程中主机I/O不断流的关键。早期开发过程中经常遇到的问题是active_processes未归零的情况，导致无法进入静默状态，从而无法触发下游的处理逻辑。处理的思路就是元数据内部逻辑的处理，整体流程为：请求到达SE，tracker++,请求到达元数据处理，元数据处理结束，返回给SE，tracker--。如果不为零，意味着有内部处理出错。

writeMode切换主要涉及到HA场景和手动切换卷owner。此处的坑主要有几个方面：
a) 控制逻辑：
b) 业务逻辑：写缓存的writeMode有wt/wb/1way-wb三种。设计三种的原因是出于写缓存的实现。首先需要定义owner。在对象管理的设计中，一个卷具有一个元数据卷对象，由一组b+树做管理。为了提高并发量，降低操作的时延，降低树的高度，在根节点的设计过程中，采用了根据lba的范围来确定树的根节点的方法，来将大树进行拆分。

## 对象管理id:


## 根节点的多线程拆分

## 大树拆分小树

使用B+树管理的原因：管理
owner在切换的过程中，首先需


5. 支持Failback功能，包括内容与4类似。
6. 支持T2恢复功能。在已有的基础上，增加了对内存数据的硬化与恢复的功能。
7. 支持离线上线功能。离线过程中的请求返回给主机相应错误码，
8. 支持离线与T1组合故障，主要涉及到对象恢复的处理与UTMD的交互。试错了两种方案，一种为将其独立到UTMD之外，自行控制agt-csm的逻辑交互，但原有的依赖于utmd的限制（如utmd的多task管理、命令行在此阶段不允许下发等）无法使用，需要很多额外的控制逻辑，未能切实地使用原有框架的设计。因此后期增加一个状态机RedoTaskSm，在之前的处理失败后返回给utmd，然后在上线后触发该状态机，重新进入utmd逻辑，以一个task来完成控制逻辑。
   9.支持T3修复功能。
# 